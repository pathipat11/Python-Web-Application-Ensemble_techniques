import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, StackingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå CSV
data = pd.read_csv('/content/data.csv')

# ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏® (Gender) ‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏£‡∏™ (Marital_Status) ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
data['Gender'] = data['Gender'].map({'Male': 0, 'Female': 1})
data['Marital_Status'] = data['Marital_Status'].map({'Single': 0, 'Married': 1})

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'Status' ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
if data['Status'].dtype == 'object':
    data['Status'] = data['Status'].astype('category').cat.codes  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
X = data[['Age', 'Length_of_Service', 'Salary', 'Gender', 'Marital_Status']]

# ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (Target)
y = data['Status']

# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∏‡∏î train ‡πÅ‡∏•‡∏∞ test (90-10)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
models = {
    "kNN": KNeighborsClassifier(n_neighbors=7),
    "Decision Tree": DecisionTreeClassifier(criterion='gini', max_depth=2),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Na√Øve Bayes": GaussianNB(var_smoothing=2e-1),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "ANN": MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=1000, random_state=42),
    "AdaBoost": AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)
}

# ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤ accuracy ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
accuracy_results = {}

# ‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
for name, model in models.items():
    model.fit(X_train, y_train)  # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    y_pred = model.predict(X_test)  # ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•
    accuracy = accuracy_score(y_test, y_pred) * 100  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå
    accuracy_results[name] = accuracy
    print(f"{name} Accuracy: {accuracy:.2f}%")

# ‡∏´‡∏≤‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
best_accuracy = max(accuracy_results.values())  # ‡∏Ñ‡πà‡∏≤ Accuracy ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
best_models = {name: models[name] for name, acc in accuracy_results.items() if acc == best_accuracy}

print("\nüèÜ Best Models:")
for name, acc in accuracy_results.items():
    if acc == best_accuracy:
        print(f"- {name} with Accuracy: {acc:.2f}%")

# **Voting Classifier** ‡∏£‡∏ß‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
VOTE_model = VotingClassifier(estimators=list(best_models.items()), voting='hard')
VOTE_model.fit(X_train, y_train)
y_pred_vote = VOTE_model.predict(X_test)
accuracy_vote = accuracy_score(y_test, y_pred_vote) * 100

print(f"\n‚úÖ Voting Classifier (Best Models Only) Accuracy: {accuracy_vote:.2f}%")

# **Stacking Classifier** ‡∏£‡∏ß‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î + Meta-Model
meta_model = RandomForestClassifier(n_estimators=100, random_state=42)  # ‡πÉ‡∏ä‡πâ Random Forest ‡πÄ‡∏õ‡πá‡∏ô Meta Model

STACK_model = StackingClassifier(estimators=list(best_models.items()), final_estimator=meta_model, passthrough=True)
STACK_model.fit(X_train, y_train)
y_pred_stack = STACK_model.predict(X_test)
accuracy_stack = accuracy_score(y_test, y_pred_stack) * 100

print(f"\nüöÄ Stacking Classifier (Ensemble of Best Models) Accuracy: {accuracy_stack:.2f}%")
